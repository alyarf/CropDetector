def save_compressed_checkpoint(state_dict, filename):
    """Save a compressed checkpoint file"""
    buffer = io.BytesIO()
    torch.save(state_dict, buffer)
    with gzip.open(filename, 'wb') as f:
        f.write(buffer.getvalue())
    print(f"Saved compressed checkpoint to {filename}")

def load_checkpoint_safely(checkpoint_path, device):
    """Safely load checkpoint with multiple fallback approaches"""
    import torch
    import gzip

    try:
        # First attempt - with weights_only=False
        if checkpoint_path.endswith('.gz'):
            with gzip.open(checkpoint_path, 'rb') as f:
                checkpoint = torch.load(f, map_location=device, weights_only=False)
        else:
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        return checkpoint

    except (RuntimeError, AttributeError) as e:
        print(f"First load attempt failed with error: {e}")

        # Enable pickle globals allowlist
        import pickle
        import torch.serialization
        torch.serialization.add_safe_globals([("numpy._core.multiarray", "_reconstruct")])

        # Second attempt - with serialization globals enabled
        if checkpoint_path.endswith('.gz'):
            with gzip.open(checkpoint_path, 'rb') as f:
                checkpoint = torch.load(f, map_location=device)
        else:
            checkpoint = torch.load(checkpoint_path, map_location=device)
        return checkpoint

def load_model_for_inference(model, checkpoint_path, device):
    """Load a model from a checkpoint for inference/testing"""
    checkpoint = load_checkpoint_safely(checkpoint_path, device)

    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"Loaded model from {checkpoint_path}")
    return model

def train_model(model, train_loader, val_loader, criterion, optimizer,  num_epochs,
                class_names, checkpoint_path=None, save_path=None, scheduler=None, start_epoch=0):
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []

    best_val_loss = float('inf')
    best_epoch = 0
    early_stop_counter = 0

    if checkpoint_path is not None and os.path.exists(checkpoint_path):
        print(f"Loading checkpoint from {checkpoint_path}")

        checkpoint = load_checkpoint_safely(checkpoint_path, device)
        model.load_state_dict(checkpoint['model_state_dict'])

        if 'optimizer_state_dict' in checkpoint and optimizer is not None:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print("Loaded optimizer state")

        if 'scheduler_state_dict' in checkpoint and scheduler is not None:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            print("Loaded scheduler state")

        Get best validation loss from checkpoint
        if 'best_val_loss' in checkpoint:
            best_val_loss = checkpoint['best_val_loss']
            print(f"Best validation loss from checkpoint: {best_val_loss:.4f}")

        Get best epoch from checkpoint
        if 'best_epoch' in checkpoint:
            best_epoch = checkpoint['best_epoch']
            print(f"Best epoch from checkpoint: {best_epoch+1}")

        if 'train_losses' in checkpoint:
            train_losses = checkpoint['train_losses']
            print(f"Loaded training loss history with {len(train_losses)} entries")

        if 'val_losses' in checkpoint:
            val_losses = checkpoint['val_losses']

        if 'train_accs' in checkpoint:
            train_accs = checkpoint['train_accs']

        if 'val_accs' in checkpoint:
            val_accs = checkpoint['val_accs']

        if 'epoch' in checkpoint:
            start_epoch = checkpoint['epoch'] + 1
            print(f"Resuming from epoch {start_epoch}")

    os.makedirs('outputs', exist_ok=True)

    for epoch in range(start_epoch, num_epochs):
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        processed_batches = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
        for i, batch in enumerate(pbar):
            torch.autograd.set_detect_anomaly(True)

            if len(batch) == 3:
                  spatial_input, target, tile_ids = batch
            else:
                  spatial_input, target = batch

            if spatial_input is None or target is None:
                continue

            spatial_input = spatial_input.to(device)
            target = target.to(device)

            optimizer.zero_grad()

            outputs=model(spatial_input)

            loss = criterion(outputs, target)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            train_loss += loss.item()
            processed_batches += 1

            with torch.no_grad():
                pred = outputs.argmax(dim=1)
                mask = (target != 0)  # Ignore class 0

                train_correct += (pred[mask] == target[mask]).sum().item()
                train_total += mask.sum().item()

            pbar.set_postfix({
                'loss': train_loss / processed_batches,
                'acc': train_correct / max(1, train_total),
                'lr': optimizer.param_groups[0]['lr']
            })

        train_loss /= max(1, processed_batches)
        train_acc = train_correct / max(1, train_total)

        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        processed_val_batches = 0

        intersection = torch.zeros(len(class_names), device=device)
        union = torch.zeros(len(class_names), device=device)

        y_true_flat = []
        y_pred_flat = []

        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]")
            for batch in pbar:
                if len(batch) == 3:
                    spatial_input, target, tile_ids = batch
                else:
                  spatial_input, target = batch

                if spatial_input is None or target is None:
                    continue

                spatial_input = spatial_input.to(device)  # [B, C, H, W]
                target = target.to(device)  # [B, H, W]

                outputs=model(spatial_input)
                loss = criterion(outputs, target)
                val_loss += loss.item()
                processed_val_batches += 1

                pred = outputs.argmax(dim=1)  # [B, H, W]

                mask = (target != 0)  # Ignore pixels with label 0
                val_correct += (pred[mask] == target[mask]).sum().item()
                val_total += mask.sum().item()

                for class_idx in range(len(class_names)):
                    pred_mask = (pred == class_idx)
                    target_mask = (target == class_idx)
                    # Collect for classification_report (flattened & masked)
                    y_true_flat.extend(target[mask].cpu().numpy().flatten())
                    y_pred_flat.extend(pred[mask].cpu().numpy().flatten())

                    intersection[class_idx] += (pred_mask & target_mask).sum()
                    union[class_idx] += (pred_mask | target_mask).sum()

        val_loss /= max(1, processed_val_batches)
        val_acc = val_correct / max(1, val_total)

        if scheduler is not None:
            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(val_loss)
            else:
                scheduler.step()

        # Store metrics for plotting
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        # Print epoch metrics
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print("\n---Classification Report ---")
        #print(classification_report(y_true_flat, y_pred_flat, target_names=class_names[1:], zero_division=0))
        y_true_np = np.array(y_true_flat)
        y_pred_np = np.array(y_pred_flat)

        mask = (y_true_np > 0)
        y_true_filtered = y_true_np[mask]
        y_pred_filtered = y_pred_np[mask]

        print(classification_report(
            y_true_filtered,
            y_pred_filtered,
            labels=[1, 2, 3, 4, 5, 6, 7],  # class indices
            target_names=class_names[1:],  # names for 1â€“4
            zero_division=0
        ))

        print(f"Current LR: {optimizer.param_groups[0]['lr']:.6f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch

            checkpoint_path = os.path.join('outputs', 'best_model.pth.gz')

            save_compressed_checkpoint({
                'epoch': epoch,
                'best_val_loss': best_val_loss,
                'best_epoch': best_epoch,

                'model_state_dict': model.state_dict(),

                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),

                'train_losses': train_losses,
                'val_losses': val_losses,
                'train_accs': train_accs,
                'val_accs': val_accs,
            }, save_path)

            print(f"Saved best model checkpoint to {save_path}")

    print("\nTraining completed!")
    print(f"Best validation loss: {best_val_loss:.4f} at epoch {best_epoch+1}")

    return {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accs': train_accs,
        'val_accs': val_accs,
        'best_epoch': best_epoch,
        'best_val_loss': best_val_loss,
        'y_true_flat': y_true_flat,
        'y_pred_flat': y_pred_flat
    }


def safe_collate(batch):
        batch = [b for b in batch if b is not None]
        return default_collate(batch)

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    train_dataset = BalancedSubsetMunich480(folder_path='/content/munich480/munich480', max_tiles_per_class=50, use_augm=True)
    val_dataset = BalancedSubsetMunich480(
         folder_path='/content/munich480/munich480',
         split="eval",
         target_size=48, use_augm=False)
   
    val_loader = DataLoader(
         val_dataset,
         batch_size=16,
         shuffle=False,
         num_workers=2,
         pin_memory=True,
         collate_fn = safe_collate
   )

    num_classes = 8
    model = ResNetUNet48(in_channels=13, num_classes=8).to(device)

    model = model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(),
          lr=1e-3,
          weight_decay=1e-4
      )

    new_class_counts = torch.tensor(
            new_train_dataset.class_pixel_counts[:8],  # ensure length-8
            dtype=torch.float
    )


    weights = 1.0 / (new_class_counts + 1e-6)
    weights = weights / weights.mean()   

    train_loader = DataLoader(
        train_dataset,
        batch_size = 16,
        sampler = sampler,
        num_workers = 2,
        pin_memory = True,
        collate_fn = None 
    )

    criterion = nn.CrossEntropyLoss(ignore_index=0, weight=class_weights)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
              optimizer, mode='min', factor=0.5, patience=5,
              threshold=1e-4, min_lr=1e-6, verbose=True
      )

    class_names = ["unknown", "meadow", "rape", "potato", "winter wheat", "winter barley", "summer barley", "maize"]


    checkpoint_path = 'outputs/save_model.pth.gz'
    save_path="outputs/save_model.pth.gz"
    start_epoch = 0
    best_val_loss = float('inf')

    history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=200,
        class_names=class_names,
        checkpoint_path=checkpoint_path if os.path.exists(checkpoint_path) else None,
        start_epoch=start_epoch,
        save_path = save_path
    )
